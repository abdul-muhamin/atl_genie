{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 5, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/src/services/chat/config.ts"],"sourcesContent":["import 'server-only'\r\n\r\nimport { Pinecone } from '@pinecone-database/pinecone'\r\n\r\nconst pinecone = new Pinecone({\r\n  apiKey: process.env.PINECONE_API_KEY!,\r\n})\r\n// const model = new OpenAI({ temperature: 0, model: \"gpt-3.5-turbo-16k\" });\r\n// const chatModel = new ChatOpenAI({\r\n//   apiKey: process.env.OPENAI_API_KEY,\r\n// });\r\nconst pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX!)\r\n\r\nexport {\r\n  // model,\r\n  pineconeIndex,\r\n}\r\n"],"names":[],"mappings":";;;;;;;;AAIA,MAAM,WAAW,IAAI,oPAAA,CAAA,WAAQ,CAAC;IAC5B,QAAQ,QAAQ,GAAG,CAAC,gBAAgB;AACtC;AACA,4EAA4E;AAC5E,qCAAqC;AACrC,wCAAwC;AACxC,MAAM;AACN,MAAM,gBAAgB,SAAS,KAAK,CAAC,QAAQ,GAAG,CAAC,cAAc"}},
    {"offset": {"line": 22, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 27, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/src/services/chat/conversation.ts"],"sourcesContent":["import { PromptTemplate } from '@langchain/core/prompts'\r\nimport { ChatOpenAI, OpenAIEmbeddings } from '@langchain/openai'\r\nimport { Document } from 'langchain/document'\r\nimport { formatDocumentsAsString } from 'langchain/util/document'\r\n\r\nimport 'server-only'\r\n\r\nimport { CacheClient, Configurations, CredentialProvider } from '@gomomento/sdk'\r\nimport { MomentoCache } from '@langchain/community/caches/momento'\r\nimport { tool } from '@langchain/core/tools'\r\nimport { HttpResponseOutputParser } from 'langchain/output_parsers'\r\n\r\nimport { pineconeIndex } from './config'\r\n\r\nexport interface Context {\r\n  role: 'user' | 'assistant' | 'system'\r\n  content: string\r\n}\r\n\r\nconst instructions = `\r\n  Act like an agent from RipeSeed, a software services company and answer the user queries accordingly.\r\n  If a user asks if we can develop something they want to, mention the projects that are similar to the user's requirements as an example.\r\n  If a user asks about particular technology/niche, check if its available in the context you have. IF available, give answers accordingly. ELSE IF NOT AVAILABLE in the context, check if a similar/niche technology is available in the context and present that to the user\r\n  If you need more information about the technologies client is looking for, feel free to ask them and narrow down the client's requirements before checking the context.\r\n  If a user asks for bugdet/timeline for a project ask them to schedule a call with ripeseed representative and also give them the RipeSee's Contact Us and Get a Quote links (https://ripeseed.io/request-a-quote).\r\n  In your response do not include the steps or logic you are taking to conclude the answer.\r\n  Your responses should include the relevant information and not the words like context, chat history, etc.\r\n  If you are mentioning multiple projects, mention them as a numbered list ONLY IF there are multiple projects.\r\n  Make sure assistant response is ALWAYS in markdown format.\r\n  Provide a paragraph where necessary, List where necessary, and code block with code language for syntax highlighting where code is needed.\r\n  Note: If user asks something NOT related to ripeseed, excuse them politely and ask them to ask the relevant questions.\r\n`\r\n\r\nconst questionPrompt = PromptTemplate.fromTemplate(\r\n  `Use the following pieces of context to answer the question at the end.\r\n----------\r\nINSTRUCTIONS: {instructions}\r\n----------\r\nCONTEXT: {context}\r\n----------\r\nCHAT HISTORY: {chatHistory}\r\n----------\r\nQUESTION: {question}\r\n----------\r\nHelpful Answer:`,\r\n)\r\n\r\nasync function initializeCache() {\r\n  try {\r\n    const client = new CacheClient({\r\n      configuration: Configurations.Laptop.v1(),\r\n      credentialProvider: CredentialProvider.fromEnvironmentVariable({\r\n        environmentVariableName: 'MOMENTO_API_KEY',\r\n      }),\r\n      defaultTtlSeconds: 60 * 60 * 24,\r\n    })\r\n\r\n    const cache = await MomentoCache.fromProps({\r\n      client,\r\n      cacheName: 'ask-ripeseed',\r\n    })\r\n\r\n    return cache\r\n  } catch (error) {\r\n    console.warn('Failed to initialize cache:', error)\r\n    return null\r\n  }\r\n}\r\n\r\nconst getMeetingTool = tool(\r\n  async () => {\r\n    return 'BOOK_MEETING'\r\n  },\r\n  {\r\n    name: 'book_meeting_call_appointment',\r\n    description:\r\n      'If someone want to talk, books calls, meetings, appointments, or any meet-up with RipeSeed',\r\n  },\r\n)\r\n\r\nconst getChain = async (apiKey: string) => {\r\n  const parser = new HttpResponseOutputParser()\r\n\r\n  // Try to initialize cache, but continue without it if it fails\r\n  const cache = await initializeCache().catch(() => null)\r\n\r\n  const chatModel = new ChatOpenAI({\r\n    ...(cache && { cache }), // Only include cache if initialization succeeded\r\n    apiKey,\r\n    model: 'gpt-4o-mini',\r\n    streaming: true,\r\n  })\r\n\r\n  const func_chatModel = chatModel.bindTools([getMeetingTool])\r\n  const chain = questionPrompt.pipe(func_chatModel).pipe(parser)\r\n\r\n  return chain\r\n}\r\n\r\nconst serializeChatHistory = (chatHistory: Context[]): string => {\r\n  return chatHistory\r\n    .map((chatMessage) => {\r\n      if (chatMessage.role === 'user') {\r\n        return `Human: ${chatMessage.content}`\r\n      } else if (chatMessage.role === 'assistant') {\r\n        return `Assistant: ${chatMessage.content}`\r\n      } else {\r\n        return `${chatMessage.content}`\r\n      }\r\n    })\r\n    .join('\\n')\r\n}\r\n\r\nexport function converse(\r\n  message: string,\r\n  context: Context[],\r\n  idArray: string[],\r\n  openAIApiKey: string,\r\n  isAskRipeseedChat: boolean = false,\r\n) {\r\n  return new ReadableStream({\r\n    async start(controller) {\r\n      const question = message\r\n\r\n      const chatHistory = await serializeChatHistory(context)\r\n\r\n      const embeddings = new OpenAIEmbeddings({ openAIApiKey })\r\n      const vector = await embeddings.embedQuery(question)\r\n\r\n      let serializedDocs = ''\r\n\r\n      if (idArray[0] !== null) {\r\n        const docs = await pineconeIndex.query({\r\n          vector,\r\n          topK: 5,\r\n          filter: { id: { $in: idArray } },\r\n          includeMetadata: true,\r\n        })\r\n\r\n        serializedDocs = formatDocumentsAsString(\r\n          docs.matches.map(\r\n            (doc) =>\r\n              new Document({\r\n                metadata: doc.metadata,\r\n                pageContent: doc.metadata?.text?.toString() || '',\r\n              }),\r\n          ),\r\n        )\r\n      }\r\n\r\n      const questionGeneratorInput = {\r\n        chatHistory,\r\n        context: serializedDocs,\r\n        question,\r\n        instructions: isAskRipeseedChat ? instructions : '',\r\n      }\r\n\r\n      const stream = (await getChain(openAIApiKey)).streamEvents(\r\n        questionGeneratorInput,\r\n        { version: 'v1' },\r\n      )\r\n\r\n      for await (const chunk of stream) {\r\n        if (chunk?.event === 'on_parser_stream') {\r\n          const data = chunk?.data.chunk\r\n          controller.enqueue(data)\r\n        } else if (chunk.event === 'on_llm_end') {\r\n          const data = chunk?.data?.output?.generations[0]\r\n          console.log('Tool end:', chunk?.data?.output?.generations[0])\r\n          if (\r\n            Array.isArray(data) &&\r\n            data[0]?.message?.tool_calls &&\r\n            data[0].message.tool_calls[0]?.name ===\r\n              'book_meeting_call_appointment'\r\n          ) {\r\n            controller.enqueue('BOOK_MEETING')\r\n          }\r\n        }\r\n      }\r\n\r\n      controller.close()\r\n    },\r\n  })\r\n}\r\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAmBA,MAAM,eAAe,CAAC;;;;;;;;;;;;AAYtB,CAAC;AAED,MAAM,iBAAiB,+WAAA,CAAA,iBAAc,CAAC,YAAY,CAChD,CAAC;;;;;;;;;;eAUY,CAAC;AAGhB,eAAe;IACb,IAAI;QACF,MAAM,SAAS,IAAI,qPAAA,CAAA,cAAW,CAAC;YAC7B,eAAe,qPAAA,CAAA,iBAAc,CAAC,MAAM,CAAC,EAAE;YACvC,oBAAoB,qPAAA,CAAA,qBAAkB,CAAC,uBAAuB,CAAC;gBAC7D,yBAAyB;YAC3B;YACA,mBAAmB,KAAK,KAAK;QAC/B;QAEA,MAAM,QAAQ,MAAM,uXAAA,CAAA,eAAY,CAAC,SAAS,CAAC;YACzC;YACA,WAAW;QACb;QAEA,OAAO;IACT,EAAE,OAAO,OAAO;QACd,QAAQ,IAAI,CAAC,+BAA+B;QAC5C,OAAO;IACT;AACF;AAEA,MAAM,iBAAiB,CAAA,GAAA,4XAAA,CAAA,OAAI,AAAD,EACxB;IACE,OAAO;AACT,GACA;IACE,MAAM;IACN,aACE;AACJ;AAGF,MAAM,WAAW,OAAO;IACtB,MAAM,SAAS,IAAI,mXAAA,CAAA,2BAAwB;IAE3C,+DAA+D;IAC/D,MAAM,QAAQ,MAAM,kBAAkB,KAAK,CAAC,IAAM;IAElD,MAAM,YAAY,IAAI,wWAAA,CAAA,aAAU,CAAC;QAC/B,GAAI,SAAS;YAAE;QAAM,CAAC;QACtB;QACA,OAAO;QACP,WAAW;IACb;IAEA,MAAM,iBAAiB,UAAU,SAAS,CAAC;QAAC;KAAe;IAC3D,MAAM,QAAQ,eAAe,IAAI,CAAC,gBAAgB,IAAI,CAAC;IAEvD,OAAO;AACT;AAEA,MAAM,uBAAuB,CAAC;IAC5B,OAAO,YACJ,GAAG,CAAC,CAAC;QACJ,IAAI,YAAY,IAAI,KAAK,QAAQ;YAC/B,OAAO,CAAC,OAAO,EAAE,YAAY,OAAO,CAAC,CAAC;QACxC,OAAO,IAAI,YAAY,IAAI,KAAK,aAAa;YAC3C,OAAO,CAAC,WAAW,EAAE,YAAY,OAAO,CAAC,CAAC;QAC5C,OAAO;YACL,OAAO,CAAC,EAAE,YAAY,OAAO,CAAC,CAAC;QACjC;IACF,GACC,IAAI,CAAC;AACV;AAEO,SAAS,SACd,OAAe,EACf,OAAkB,EAClB,OAAiB,EACjB,YAAoB,EACpB,oBAA6B,KAAK;IAElC,OAAO,IAAI,eAAe;QACxB,MAAM,OAAM,UAAU;YACpB,MAAM,WAAW;YAEjB,MAAM,cAAc,MAAM,qBAAqB;YAE/C,MAAM,aAAa,IAAI,uWAAA,CAAA,mBAAgB,CAAC;gBAAE;YAAa;YACvD,MAAM,SAAS,MAAM,WAAW,UAAU,CAAC;YAE3C,IAAI,iBAAiB;YAErB,IAAI,OAAO,CAAC,EAAE,KAAK,MAAM;gBACvB,MAAM,OAAO,MAAM,mIAAA,CAAA,gBAAa,CAAC,KAAK,CAAC;oBACrC;oBACA,MAAM;oBACN,QAAQ;wBAAE,IAAI;4BAAE,KAAK;wBAAQ;oBAAE;oBAC/B,iBAAiB;gBACnB;gBAEA,iBAAiB,CAAA,GAAA,oWAAA,CAAA,0BAAuB,AAAD,EACrC,KAAK,OAAO,CAAC,GAAG,CACd,CAAC,MACC,IAAI,mXAAA,CAAA,WAAQ,CAAC;wBACX,UAAU,IAAI,QAAQ;wBACtB,aAAa,IAAI,QAAQ,EAAE,MAAM,cAAc;oBACjD;YAGR;YAEA,MAAM,yBAAyB;gBAC7B;gBACA,SAAS;gBACT;gBACA,cAAc,oBAAoB,eAAe;YACnD;YAEA,MAAM,SAAS,CAAC,MAAM,SAAS,aAAa,EAAE,YAAY,CACxD,wBACA;gBAAE,SAAS;YAAK;YAGlB,WAAW,MAAM,SAAS,OAAQ;gBAChC,IAAI,OAAO,UAAU,oBAAoB;oBACvC,MAAM,OAAO,OAAO,KAAK;oBACzB,WAAW,OAAO,CAAC;gBACrB,OAAO,IAAI,MAAM,KAAK,KAAK,cAAc;oBACvC,MAAM,OAAO,OAAO,MAAM,QAAQ,WAAW,CAAC,EAAE;oBAChD,QAAQ,GAAG,CAAC,aAAa,OAAO,MAAM,QAAQ,WAAW,CAAC,EAAE;oBAC5D,IACE,MAAM,OAAO,CAAC,SACd,IAAI,CAAC,EAAE,EAAE,SAAS,cAClB,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,UAAU,CAAC,EAAE,EAAE,SAC7B,iCACF;wBACA,WAAW,OAAO,CAAC;oBACrB;gBACF;YACF;YAEA,WAAW,KAAK;QAClB;IACF;AACF"}},
    {"offset": {"line": 188, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 193, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/src/app/api/chat/send-message/route.ts"],"sourcesContent":["import { converse } from '@/services/chat/conversation'\r\n\r\nexport async function POST(request: Request, response: Response) {\r\n  try {\r\n    // indexId is the id of the document index\r\n    const { apiKey, messages, indexId, chatId } = await request.json()\r\n    // Documents chat\r\n    const streamedResponse = await converse(\r\n      messages[messages.length - 1].content,\r\n      messages,\r\n      [indexId],\r\n      apiKey,\r\n    )\r\n    return new Response(streamedResponse, {\r\n      headers: { 'Content-Type': 'text/plain; charset=utf-8' },\r\n    })\r\n  } catch (err) {\r\n    if (err instanceof Error) {\r\n      return new Response(err.message, { status: 400 })\r\n    }\r\n    return new Response('Something went wrong', { status: 500 })\r\n  }\r\n}\r\n"],"names":[],"mappings":";;;;;;AAEO,eAAe,KAAK,OAAgB,EAAE,QAAkB;IAC7D,IAAI;QACF,0CAA0C;QAC1C,MAAM,EAAE,MAAM,EAAE,QAAQ,EAAE,OAAO,EAAE,MAAM,EAAE,GAAG,MAAM,QAAQ,IAAI;QAChE,iBAAiB;QACjB,MAAM,mBAAmB,MAAM,CAAA,GAAA,yIAAA,CAAA,WAAQ,AAAD,EACpC,QAAQ,CAAC,SAAS,MAAM,GAAG,EAAE,CAAC,OAAO,EACrC,UACA;YAAC;SAAQ,EACT;QAEF,OAAO,IAAI,SAAS,kBAAkB;YACpC,SAAS;gBAAE,gBAAgB;YAA4B;QACzD;IACF,EAAE,OAAO,KAAK;QACZ,IAAI,eAAe,OAAO;YACxB,OAAO,IAAI,SAAS,IAAI,OAAO,EAAE;gBAAE,QAAQ;YAAI;QACjD;QACA,OAAO,IAAI,SAAS,wBAAwB;YAAE,QAAQ;QAAI;IAC5D;AACF"}},
    {"offset": {"line": 223, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}}]
}